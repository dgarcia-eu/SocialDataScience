---
title: "Running Unsupervised Sentiment Analysis in R"
author: "Dr. David Garcia"
output: html_document
---

The R community has developed a lot of packages to run off-the-shelf unsupervised sentiment analysis methods, also called dictionary methods. Once a method is published for another language (e.g. Python), it is just a matter of time that an R developer does the R version of the package. Furthermore, there are useful packages to access dictionaries and other useful resources directly from R. In this tutorial, you will see some examples of how to use these resources. You can find the sources to run this yourself in the associated [Github repostory](https://github.com/dgarcia-eu/SocialDataScience/tree/master/3_Affect/035_UnsupervisedToolsR).

### VADER

As we saw in the [unsupervised sentiment analysis](https://dgarcia-eu.github.io/SocialDataScience/3_Affect/032_UnsupervisedSentimentAnalysis/UnsupervisedSentimentAnalysis.html) topic, [VADER](https://github.com/cjhutto/vaderSentiment) is one of the best unsupervised methods to analyze social media text, in particular Twitter. A recently developed R package makes it very easy to run VADER from R. To install and load the package, you just need to do as with any other R package:

```{r eval=F}
install.packages("vader")
```

```{r}
library(vader)
```

The vader package includes a function called get_vader() that will run the VADER method over a text you give it:

```{r}
get_vader(":) ")
```
The result is an R vector with named entries:  
- word_scores: a string that contains an ordered list with the matched scores for each of the words in the text. In the example you can see the negative score for "horrible" and the postive score for "love".
- compound: the final valence compound of VADER for the whole text after applying modifiers and aggregation rules   
- pos, neg, and neu: the parts of the compound for positive, negative, and neutral content. These take into account modifiers and are combined when calculating the compound score 
- but_count: an additional count of "but" since it can complicate the calculation of sentiment

A named vector can be accessed with the bracket operator:
```{r}
vaderres <- get_vader("This book is horrible, but I love it.")
vaderres["compound"]
```

VADER takes into account some punctuation signs, for example an exclamation sign makes a positive word more intense here:
```{r}
get_vader("This book is horrible, but I love it!")
```

Words can also modify the meaning of other words, for example when amplifiers make it stronger:
```{r}
get_vader("This book is bad")
get_vader("This book is very bad")
```

Modifiers can make sentiment weaker too, for example the word "slightly" in this case:
```{r}
get_vader("This book is bad")
get_vader("This book is slightly bad")
```

And negators reverse the valence of a word and weaken it a bit, based on empirical observations reported in the [original paper](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/view/8109):
```{r}
get_vader("This book is not bad")
```

**Your turn**

Let's break VADER! explore some sentences that VADER might analyze wrong in terms of valence. Can you tell why VADER does or does not work in those cases?
```{r}
# Your code here
```


The vader package also includes a vader_df() function to run VADER over a series of texts and produce results in a data frame. Here we use the schrute library to load the scripts of the US TV series "The Office" and run VADER over the first six lines of the series:

```{r}
library(schrute)
texts <- head(theoffice$text)
vader_df(texts)
```

Due to the modification rules, VADER is not as fast as other dictionary-based methods and running it for the whole theoffice dataset can take from minutes to hours depending on your computer. However, the vader_df function makes it much faster than a loop.

### Syuzhet

Syuzhet is a dictionary developed by (Matthew Jockers](https://www.matthewjockers.net/2015/02/02/syuzhet/) to analyze the arcs of sentiment in novels. The R package syuzhet makes it very easy to use and includes other sentiment dictionaries too:  

```{r eval=F}
install.packages("syuzhet")
```

```{r}
library(syuzhet)
```

The syuzhet package provides the get_sentiment() function that works the same way as the get_vader() function, but runs the syuzhet lexicon and computes a mean over the words of the text.

**Your turn**

Run get_sentiment over a few sentences. As with VADER, let's try to break Syuzhet and find examples that get the wrong score. Is it easier or harder than with VADER?

```{r}
# Your code here
```


The get_sentiment() function can be run for a vector with more than one text, making it simple to get many scores:
```{r}
get_sentiment(head(theoffice$text))
```

syuzhet also includes other dictionaries, like the popular [AFINN dictionary](https://finnaarupnielsen.wordpress.com/2011/03/16/afinn-a-new-word-list-for-sentiment-analysis/). In AFINN, words are scored in the same scale as SentiStrength, as integers from -5 to +5, and thus the output looks a bit different:
```{r}
get_sentiment(head(theoffice$text), method="afinn")
```

When combined with dplyr data wrangling tools, packages like syuzhet make it very easy to compute sentiment aggregates. Here we run Syuzhet over all the lines in The Office, since it's much faster than VADER. We make a ranking by mean sentiment over the main characters of the series:
```{r}
library(dplyr)
theoffice$sentiment <- get_sentiment(theoffice$text)
theoffice %>% 
  group_by(character) %>% 
  summarise(sent=mean(sentiment), n=n()) %>% 
  arrange(desc(n)) %>% head(n=20) %>% 
  arrange(desc(sent))
```

### Tidytext

tidytext is a very useful package that follows the same philosophy as dplyr but for text. It is very powerful for text analysis, including sentiment analysis.

```{r eval=F}
install.packages("tidytext")
```

```{r}
library(tidytext)
```

When you run some tidytext functions, it might ask you to install additional packages like "textdata" or download resources like the sentiment dictionaries. The plain installation keeps these additional resources to a minimum and you will be asked to install them only once the first time you use them.

Tidytext, as other dplyr-related packages, usest tibbles, which is an upgraded version of a data frame. Converting plain text to a tibble is rather simple. In this example we create a tibble with the text of the lines of the theoffice dataset and add a column with the name of the character that says the line:

```{r}
texts <- theoffice$text
head(texts)

textdf <- tibble(character=theoffice$character, text=texts)
head(textdf)
```

Once you have your text in a tibble, you can tokenize the texts (i.e. separating it into words) with the  unnest_tokens function. This function creates a column with the name of its first parameter to put the tokens of the text specified in its second parameter:

```{r}
textdf %>% 
  unnest_tokens(word, text) -> wordsdf
head(wordsdf)
```

You can combine it with the count() function of dplyr to make a table of word frequencies in the whole theoffice corpus:

```{r}
wordsdf %>% 
  count(word) %>% 
  arrange(desc(n))
```

Tidytext includes some useful datasets, for example the stop_words dataset includes stop words that barely contain meaning in English. You can combine it with the anti_join() function to remove them from the tokens tibble:
```{r}
data(stop_words)
wordsdf %>% 
  anti_join(stop_words)
```

You can see the effect of removing stopwords in the ranking we calculated earlier:
```{r}
wordsdf %>% 
  anti_join(stop_words)  %>% 
  count(word) %>% 
  arrange(desc(n))
```

You can do this for any word list, you can specify your own one and count occurences of words in the list. For example, you can calculate the normalized frequency of the words "uh", "hey", and "um" across characters that say more than 5000 words:

```{r}
wordlist <- data.frame(word=c("uh","hey","um"))
wordsdf %>% 
  inner_join(wordlist)  %>% 
  group_by(character) %>%
  summarize(nuh=n()) -> uhcount

wordsdf %>% 
  group_by(character) %>%
  summarize(n=n()) -> charcount

inner_join(uhcount,charcount) %>% filter(n>5000) %>% mutate(ratioUh = nuh/n) %>% arrange(desc(ratioUh))
```

You can learn more about tidytext at https://www.tidytextmining.com/

### Using your own lexicon

You can run yourself any dictionary method through tidytext as long as you have a file with the words and their scores or sentiment classifications. tidytext includes three sentiment lexica with three different kind of annotations. You can get the annotations of each word with the get_sentiments() function. When running the lines below, you will have to say "yes" to download the data files:

```{r}
get_sentiments("afinn")
```

```{r}
get_sentiments("bing")
```

As you see, AFINN has a numerical score per word, bing has words in classes of positivity and negativity, and the NRC lexicon maps words to emotions (these are emotions from Plutchik's wheel, you can learn more about it in the [measuring emotions topic](https://dgarcia-eu.github.io/SocialDataScience/3_Affect/031_MeasuringEmotions/Emotions.html)).

You can easily match words in texts to these lexica with tidytext. The code below takes the tokens data frame, matches words with the corresponding scores in the AFINN lexicon, and calculates a ranking of main characters in The Office by the mean sentiment score of the words they say.

```{r}
wordsdf %>% 
  inner_join(get_sentiments("afinn")) %>%
  group_by(character) %>%
  summarize(sent=mean(value), n=n()) %>%
  arrange(desc(n)) %>%
  head(20) %>%
  arrange(desc(sent)) -> afinndf

afinndf
```
As you see, even the most negative character (Stanley) still has a positive mean sentiment score. This is called the postivity bias of language and you can see more examples about it if you run other sentiment analysis methods.

in the [Twitter sentiment exercise](https://dgarcia-eu.github.io/SocialDataScience/3_Affect/038_TwitterSentiment/Twitter_Sentiment.html).

You can do the same as above but based on a file. [Saif Mohammad](https://www.saifmohammad.com/) developed the NRC lexica and one of the latests is the [NRC Valence, Arousal, and Dominance lexicon (NRC-VAD)](https://saifmohammad.com/WebPages/nrc-vad.html). The following code downloads the NRC-VAD files and unzips them in your local folder.

```{r, eval=F}
#download.file("https://saifmohammad.com/WebDocs/VAD/NRC-VAD-Lexicon-Aug2018Release.zip", destfile="NRCVAD.zip")
#unzip("NRCVAD.zip")
```

To use the lexicon, we have to read the valence file in English, name its columns, and convert it into a tibble:
```{r}
Valencedf <- read.table("NRC-VAD-Lexicon-Aug2018Release/OneFilePerDimension/v-scores.txt", header=F, sep="\t")
names(Valencedf) <- c("word","valence")
vdf <- tibble(Valencedf)
```

Then in the same way as when we used AFINN, we can calculate the mean valence of words said by each character in The Office:
```{r}
wordsdf %>% 
  inner_join(vdf) %>%
  group_by(character) %>%
  summarize(meanvalence=mean(valence), n=n()) %>%
  arrange(desc(n)) %>%
  head(20) %>%
  arrange(desc(meanvalence)) -> nrcdf
nrcdf
```

**Your turn**

Compare the mean valence values for AFINN and the NRC-VAD lexicon. Are they correlated?

```{r}
# Your code here
```

### Sentiment analysis with Transformer models

**Run this part in an interactive R console rather than in the markdown file or knitting**

You can use off-the-shelf state-of-the-art sentiment analysis models based on the Transformers architecture (which ChatGPT uses). You need to install the reticulate package, which allows you to install a way to use python code within R called miniconda. You only need to run this install_miniconda once and it will be set up.
```{r, eval=F}
install.packages("reticulate")
library(reticulate)
install_miniconda()
```

Once you have that set up, you can install the latest version of the huggingfaceR package, which lets you use models stored in the HuggingFace repository. To set up this link to HuggingFace, you need to run hf_python_depends() to download dependencies within Python, so you can just focus on your R code.
```{r, eval=F}
devtools::install_github("farach/huggingfaceR")
library(huggingfaceR)
hf_python_depends() 
```

Now you can download and run models from HuggingFace. For example, you can use DistilBERT for text classification into sentiment:
```{r, eval=F}
library(huggingfaceR)

distilBERT <- hf_load_pipeline(
	model_id = "distilbert-base-uncased-finetuned-sst-2-english", 
	task = "text-classification"
	)

distilBERT("I like you. I love you")
```

Or LEIA (Linguistic Embeddings for the Identification of Affect) for emotion identification:
```{r, eval=F}
LEIA <- hf_load_pipeline(
	model_id = "LEIA/LEIA-base", 
	task = "text-classification"
	)

LEIA("I am so angry right now.")
```

[If you want to learn more about LEIA, check our recent paper about it.](https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-023-00427-0)